num_classes: &num_classes 1000

runtime:
  random_seed: 123

random_resized_crop: &random_resized_crop
 type: torch_random_resized_crop
 kwargs:
   size: 224
   scale: [0.08, 1]

random_horizontal_flip: &random_horizontal_flip
 type: torch_random_horizontal_flip

pil_color_jitter: &pil_color_jitter
 type: torch_color_jitter
 kwargs:
  brightness: 0.4
  contrast: 0.4
  saturation: 0.4
  hue: 0.0

center_crop: &center_crop
  type: torch_center_crop
  kwargs:
    size: 224

torch_size: &torch_resize
  type: torch_resize
  kwargs:
    size: 256

to_tensor: &to_tensor
  type: to_tensor

normalize: &normalize
 type: normalize
 kwargs:
   mean: [0.485, 0.456, 0.406] # ImageNet pretrained statics
   std: [0.229, 0.224, 0.225]

dataset: # Required.
  train:
    dataset:
      type: cls
      kwargs:
        meta_file: datasets/imagenet_lt/ImageNet_LT_train.txt
        image_reader:
           type: fs_pillow
           kwargs:
             image_dir: /mnt/lustre/share/images
             color_mode: RGB
        transformer: [*random_resized_crop, *random_horizontal_flip, *pil_color_jitter, *to_tensor, *normalize]
  test:
    dataset:
      type: cls
      kwargs:
        meta_file: datasets/imagenet_lt/ImageNet_LT_test.txt 
        image_reader:
           type: fs_pillow
           kwargs:
             image_dir: /mnt/lustre/share/images
             color_mode: RGB
        transformer: [*torch_resize, *center_crop, *to_tensor, *normalize]
        save_score: True
        evaluator:
          type: imagenet_lt               # choices = {'COCO', 'VOC', 'MR'}
          kwargs:
            topk: [1, 5]
            cls_freq: datasets/imagenet_lt/imagenet_lt_trainnums.json
            many_shot_thr: 100
            low_shot_thr: 20
  batch_sampler:
    type: base
    kwargs:
      sampler:
        type: dist
        kwargs: {}
      batch_size: 32
  dataloader:
    type: cls_base
    kwargs:
      num_workers: 12
      pin_memory: True

trainer: # Required.
  max_epoch: 90
  test_freq: 10
  save_freq: 10
  only_save_latest: True
  optimizer:                 # optimizer = SGD(params,lr=0.001,momentum=0.9,weight_decay=0.0001)
    type: SGD
    kwargs:
      lr: 0.1
      momentum: 0.9
      nesterov: True
      weight_decay: 0.0005
  lr_scheduler:              # lr_scheduler = MultStepLR(optimizer, milestones=[9,14],gamma=0.1)
    #warmup_iter: 2500          # 1000 iterations of warmup
    #warmup_type: linear
    warmup_register_type: no_scale_lr
    #warmup_ratio: 0.25
    type: EpochBasedCosineAnnealingLR
    kwargs:
      T_max: 90
      eta_min: 0.0
      offset_epoch: 0

saver: # Required.
  save_dir: checkpoints/cls_std     # dir to save checkpoints
  results_dir: results_dir/cls_std  # dir to save detection results. i.e., bboxes, masks, keypoints
  auto_resume: True  # find last checkpoint from save_dir and resume from it automatically
                     # this option has the highest priority (auto_resume > opts > resume_model > pretrain_model)

hooks:
  - type: auto_save_best
  - type: gradient_collector
    kwargs:
      hook_module: [post_process.cls_loss]
      collect_func_module: post_process.cls_loss
      grad_type: input

net: &subnet
  - name: backbone              # backbone = resnet50(frozen_layers, out_layers, out_strides)
    type: resnet50
    kwargs:
      frozen_layers: []
      out_layers: [4]     # layer1...4, commonly named Conv2...5
      out_strides: [32]  # tell the strides of output features
      normalize:
        type: solo_bn
      initializer:
        method: msra
  - name: head
    type: base_cls_head
    kwargs:
       num_classes: *num_classes
       in_plane: &head_out_channel 2048
       input_feature_idx: -1
       classifier_cfg:
         type: norm_linear
         kwargs:
           tempearture: 30
  - name: post_process
    type: base_cls_postprocess
    kwargs:
       cls_loss:
         type: softmax_eql
         kwargs:
            num_classes: *num_classes
            indicator: pos
            tau: 1.0
